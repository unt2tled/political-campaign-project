run training and check training loss
check if it really reaches
add explanation for loss of data
try to split test to parts as in the train and valid
at least few thousand datapoints
dominant words as preditor method
simpler modalities as music or face recognition or improving text model
more data should improve results
summarization of text: open problem with models
use bert instead of distilbert
extract from one website

action items:
1. training log (loss) - ask Edo about the meaning of our results of training Vix - Maor - V
2. splits to 3 parts also for the test - Dean - V
3. pre-training on similar text from websites - Mark (take your time) - not so crucial right now
4. summarization (as part of summarization network. Should be efficient.) - Maor (Dean assistant)
5. counting words distriburion - Mark (easy peasy)
